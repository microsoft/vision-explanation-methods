{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import PIL.Image as Image\n",
    "\n",
    "from matplotlib import pyplot as pl\n",
    "\n",
    "import torchvision.models.detection as d\n",
    "\n",
    "import base64\n",
    "import io\n",
    "\n",
    "import PIL.Image as Image\n",
    "\n",
    "from matplotlib import pyplot as pl\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import vision_explanation_methods\n",
    "import vision_explanation_methods.DRISE_runner as dr\n",
    "from ml_wrappers.model.image_model_wrapper import (PytorchDRiseWrapper, WrappedObjectDetectionModel)\n",
    "from vision_explanation_methods.explanations import drise\n",
    "\n",
    "import numpy as np\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torchvision\n",
    "from captum.attr import visualization as viz\n",
    "from ml_wrappers.model.image_model_wrapper import (MLflowDRiseWrapper,\n",
    "                                                   PytorchDRiseWrapper)\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models import detection\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from vision_explanation_methods.explanation_evaluation.pointing_game import PointingGame\n",
    "try:\n",
    "    from matplotlib.axes._subplots import AxesSubplot\n",
    "except ImportError:\n",
    "    # For matplotlib >= 3.7.0\n",
    "    from matplotlib.axes import Subplot as AxesSubplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get path of an example image\n",
    "BASE_DIR = \"../images/\"\n",
    "img_fname = os.path.join(BASE_DIR, \"2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get fasterrcnn model\n",
    "model = d.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "detection_model = PytorchDRiseWrapper(model=model, number_of_classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find saliency scores for top 20% of salient pixels\n",
    "pg = PointingGame(detection_model)\n",
    "s = pg.pointing_game(img_fname, 0, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the saliency map for highly salient pixels\n",
    "test_image = Image.open(img_fname).convert('RGB')\n",
    "figure = pg.visualize_highly_salient_pixels(test_image, s)\n",
    "figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find what percent of salient pixels overlap with the area of the ground truth bbox\n",
    "# each channel of the saliency scores matrix should be the same, which is why you can\n",
    "# just take the first index of s\n",
    "overlap = pg.calculate_gt_salient_pixel_overlap(s[0], [247, 192, 355, 493])\n",
    "overlap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_rotation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
