{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQr10m-RqESO"
      },
      "source": [
        "## Microsoft Object Detection Explainability (D-RISE) Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaGsHfDLrvcb"
      },
      "source": [
        "Authors: Raphi Kang, Shreya Ravikumar. 2023 internship at Microsoft Explainable AI Team, with Ilya Matiach, Jimmy Hall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD2OOiIwrJ5P"
      },
      "source": [
        "This tutorial will walk us through the process of fine-tuning a pretrained object detection model, then generating saliency maps with the [D-RISE (Detector Randomized Input Sampling for Explanation)](https://arxiv.org/abs/2006.03204) method. The same results can also be achieved by using our python package [vision-explanation-methods](https://github.com/microsoft/vision-explanation-methods).\n",
        "D-RISE is a black-boxed, or model-agnostic, explainability method which can produce saliency maps for any object detection or instance segmentation models provided these models are appropriately wrapped. In essence, D-RISE works by random masking the input images and isolating the parts that are most pertinent for the detection or segmentation of the object in question.  In this implementation, we introduce a custom wrapper class for PyTorch FastRCNN models, and provide the option for the user to define their own wrapper class.\n",
        "\n",
        "First we install all necessary dependencies and import them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IB649CTtiUk",
        "outputId": "19b20444-74cb-439c-8f9f-aa1f48fea418"
      },
      "outputs": [],
      "source": [
        "!pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nznQgJjItwrP",
        "outputId": "52ff91ba-77ca-457e-f575-20674386822e"
      },
      "outputs": [],
      "source": [
        "!pip install BeautifulSoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_SMoojf74Qm"
      },
      "source": [
        "We will also install the vision-explanation-methods package for access its dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_0QYNAWtqqK",
        "outputId": "e0a21318-e69a-484a-8c07-230ff5e5b496"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/microsoft/vision-explanation-methods.git#subdirectory=python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O00011F2HhR2"
      },
      "source": [
        "# 1. Fine-tuning an Object Detection Model\n",
        "\n",
        "If you already have a pretrained model you would like to load, you can skip to the next portion. In this section, we will be showing how to finetune a Faster R-CNN model on a recycling dataset. Many of the steps follow the TorchVision Object Detection Finetuning tutorial. (https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeYqYO0JQ9Eh"
      },
      "source": [
        "### Recyling finetuning - creating the dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQyP0ncSktQ9"
      },
      "source": [
        "First, let's download and extract the data from Microsoft computer-vision recipes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kQllD0PsSCKc",
        "outputId": "e5e19fc8-b05a-4887-ef5c-63c4aa07b4a4"
      },
      "outputs": [],
      "source": [
        "# download the Fridge objects dataset from Microsoft computer-vision recipes\n",
        "!wget https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip .\n",
        "# extract it in the current folder\n",
        "!unzip odFridgeObjects.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hJkwITc1FlI"
      },
      "source": [
        "Now let's write a custom dataset for the odFridgeObjects dataset with data augmentation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jRnif1sB5q5N"
      },
      "outputs": [],
      "source": [
        "#@title Data augmentation \n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "# Set train=True for training transforms and False for val/test transforms\n",
        "def get_transform(train):\n",
        "    if train:\n",
        "        return A.Compose([A.HorizontalFlip(0.5),\n",
        "                            A.VerticalFlip(p=0.2),\n",
        "                            A.CropAndPad(percent = (0, 0.1)),\n",
        "                            A.GaussianBlur(sigma_limit = 0.05, p = 0.5),\n",
        "                            A.RandomBrightnessContrast(brightness_limit = 0.05, contrast_limit = 0.05, p = 0.5),\n",
        "                            A.Affine(\n",
        "                                    scale= {\"x\": (1, 1.5), \"y\": (1, 1.5)},\n",
        "                                    translate_percent={\"x\": (0, 0.05), \"y\": (0, 0.05)},\n",
        "                                    rotate=[-3, 3],\n",
        "                            ),\n",
        "                            ToTensorV2(p=1.0)\n",
        "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "    else:\n",
        "        return A.Compose([\n",
        "                            ToTensorV2(p=1.0)\n",
        "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MPTQU_V-1cr9"
      },
      "outputs": [],
      "source": [
        "#@title Defining the Fridge Dataset class \n",
        "import torch\n",
        "import torch.utils.data\n",
        "import cv2\n",
        "from bs4 import BeautifulSoup \n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "class FridgeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images/\"))))[2:]\n",
        "        self.boxes = list(sorted(os.listdir(os.path.join(root, \"annotations/\"))))[2:]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images \n",
        "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
        "        box_path = os.path.join(self.root, \"annotations\", self.boxes[idx])\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # get bounding box coordinates and labels for each annotation\n",
        "        with open(box_path, 'r') as f:\n",
        "            data = f.read() \n",
        "\n",
        "        bs_data = BeautifulSoup(data, 'xml') \n",
        "        xmin = bs_data.find_all('xmin') \n",
        "        xmax = bs_data.find_all('xmax') \n",
        "        ymin = bs_data.find_all('ymin') \n",
        "        ymax = bs_data.find_all('ymax') \n",
        "\n",
        "        label = bs_data.find_all('name') \n",
        "        label_dict = {'can': 1, 'carton': 2, 'milk_bottle': 3, 'water_bottle': 4}\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for i in range(0,len(xmin)):\n",
        "            box = [int(xmin[i].get_text()),int(ymin[i].get_text()),\n",
        "                    int(xmax[i].get_text()),int(ymax[i].get_text())]\n",
        "            boxes.append(box)\n",
        "            labels.append(label_dict[label[i].get_text()])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((len(label),), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        # apply transforms to \n",
        "        if self.transforms:\n",
        "            transform = self.transforms(image = img,\n",
        "                                    bboxes = target['boxes'],\n",
        "                                    labels = target['labels'])\n",
        "                \n",
        "            img = transform['image']\n",
        "            target['boxes'] = torch.Tensor(transform['bboxes'])\n",
        "                \n",
        "        img = F.convert_image_dtype(img, torch.float)\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_l0cmG3Gazs"
      },
      "source": [
        "### Recyling finetuning - training the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rjOnNDgkq2s"
      },
      "source": [
        "First, we need to install all the necessary dependencies and import them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV-7vl2HOOIV",
        "outputId": "105d92f9-188b-48f1-9364-e2804c634483"
      },
      "outputs": [],
      "source": [
        "!pip install cython\n",
        "# Install pycocotools, the version by default in Colab\n",
        "# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-1WTeaAQWBz"
      },
      "source": [
        "\n",
        "There are helper functions to simplify training and evaluating object detection models in the references/detection/ subdirectory of TorchVision. Here, we will use references/detection/engine.py, references/detection/utils.py and references/detection/transforms.py.\n",
        "\n",
        "Let's copy those files (and their dependencies) in here so that they are available in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT9xepfNP2sk",
        "outputId": "edb42474-250d-4411-abc8-26c24a9a231e"
      },
      "outputs": [],
      "source": [
        "# Download TorchVision repo to use some files from references/detection\n",
        "!git clone https://github.com/pytorch/vision.git\n",
        "!cd vision\n",
        "!cp ./vision/references/detection/utils.py ./\n",
        "!cp ./vision/references/detection/transforms.py ./\n",
        "!cp ./vision/references/detection/coco_eval.py ./\n",
        "!cp ./vision/references/detection/engine.py ./\n",
        "!cp ./vision/references/detection/coco_utils.py ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_aFeSKaV_xn"
      },
      "source": [
        "Then, we need to define the model. Here are examples of how to load the  Faster R-CNN and RetinaNet models.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6tCnm1jiWPUg"
      },
      "outputs": [],
      "source": [
        "#@title Loading the model\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "    \n",
        "def get_instance_segmentation_model(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "509fcabd55ac4ed091e495caf15342ee"
          ]
        },
        "id": "kN_XDbPk0v5F",
        "outputId": "1b310934-d4e7-4f84-c406-0c8f6a186203"
      },
      "outputs": [],
      "source": [
        "model = get_instance_segmentation_model(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmRCHW4ptIYg"
      },
      "source": [
        "Finally, let's train and save our finetuned model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmEvbUuoOGX-",
        "outputId": "454656e3-6580-41c1-a46c-2383e6b69771"
      },
      "outputs": [],
      "source": [
        "#@title Training the model\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# the fridge dataset has five classes only \n",
        "num_classes = 5 \n",
        "\n",
        "# use our dataset and defined transformations\n",
        "dataset = FridgeDataset('odFridgeObjects', get_transform(train=True))\n",
        "dataset_test = FridgeDataset('odFridgeObjects', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-20])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-20:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_instance_segmentation_model(num_classes)\n",
        "# model = get_retinanet_model(num_classes) # uncomment to use retinanet model\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=3,\n",
        "                                                gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIX7O-nZ26JN",
        "outputId": "e94b5bb1-2327-437f-c143-588282ce553b"
      },
      "outputs": [],
      "source": [
        "# let's train it for 10 epochs\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "print(\"That's it!\")\n",
        "torch.save(model.state_dict(), 'Recycling_finetuned_FastRCNN.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDLBE_jUH8OW"
      },
      "source": [
        "# 2. Loading in your pre-trained model and test image\n",
        "\n",
        "Next, we load in our pretrained model and visualize its detections on some test image through D-RISE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ4xdrTd-Xgg"
      },
      "outputs": [],
      "source": [
        "import urllib.request as request_file\n",
        "\n",
        "#download fine-tuned recycling model from url\n",
        "def download_assets(filepath,force=False):\n",
        "    if force or not os.path.exists(filepath):\n",
        "        request_file.urlretrieve(\n",
        "                        \"https://publictestdatasets.blob.core.windows.net/models/fastrcnn.pt\",\n",
        "                        os.path.join(filepath))\n",
        "    else:\n",
        "        print('Found' + filepath)\n",
        "\n",
        "    return filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0a48079bd32c4b68af373753a6e89524",
            "f0d9c2dda8224d08a00347ad85761ded",
            "75802c4660524073a07a6f5115e2a375",
            "d13c6cf681e848558e5b2e0f542c5f60",
            "303743a5454f44f49ab3d4701c04beb5",
            "54ab686aacb244af8e26d86a628815e8",
            "1966e4a247fb47e19a31901696a62dbe",
            "52f85758e0064f46a414a51506e9040b",
            "8986f48e2e1f47988cd55dfa68fef651",
            "c7060c74f47e4c1a8d3acb7f4159ee93",
            "a36adc2dccd24407a0dccd854905f1c3"
          ]
        },
        "id": "Qcchc5xxH-iA",
        "outputId": "190a7779-e772-48e9-eabb-bdd0dbb2dffb"
      },
      "outputs": [],
      "source": [
        "#Loading in our pretrained model     \n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def get_instance_segmentation_model(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_classes = 5\n",
        "model = get_instance_segmentation_model(num_classes)\n",
        "_ = download_assets('Recycling_finetuned_FastRCNN.pt')\n",
        "model.load_state_dict(torch.load('Recycling_finetuned_FastRCNN.pt', map_location = device))\n",
        "\n",
        "#if using the general torchvision pretrained model, comment above and uncomment below\n",
        "# model = detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "EdHDcArUr6eb",
        "outputId": "52d155c6-b9b7-4626-97ac-4ef4f48a69c1"
      },
      "outputs": [],
      "source": [
        "#load in a test image\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms as T\n",
        "\n",
        "test_image = Image.open('odFridgeObjects/images/17.jpg')\n",
        "plt.imshow(test_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwjGbJWmsacJ"
      },
      "source": [
        "Without any additional wrapping, the output of this model looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnZwgwtosayP"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "detections = model(T.ToTensor()(test_image).to(device).unsqueeze(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDpM8SVhxMho"
      },
      "source": [
        "\n",
        "# 3. Setting up D-RISE Model Wrapper\n",
        "\n",
        "D-RISE expects a common model output format, which is a list of \"Detection Records\". The wrapper for Faster R-CNN is written here (and it's the default option in the package) - if your model is of a different type, you have to write a wrapper to match this output type before you continue. Writing a customized wrapper should be pretty straightforward - the only requirement is that the wrapped model output should return a list of DetectonRecords, with attributes bounding_boxes, class_scores, and objectness_scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN7P5FOmwvMW"
      },
      "outputs": [],
      "source": [
        "from vision_explanation_methods.explanations import common as od_common\n",
        "\n",
        "class PytorchFasterRCNNWrapper(od_common.GeneralObjectDetectionModelWrapper):\n",
        "    \"\"\"Wraps a PytorchFasterRCNN model with a predict API function for object detection.\n",
        "\n",
        "    To be compatible with the drise explainability method, all models must be wrapped to have\n",
        "    the same output and input class.\n",
        "    This wrapper is customized for the FasterRCNN model from Pytorch, and can\n",
        "    also be used with the RetinaNet or any other models with the same output class.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, number_of_classes: int):\n",
        "        self._model = model\n",
        "        self._number_of_classes = number_of_classes\n",
        "\n",
        "    def predict(self, x: torch.Tensor):\n",
        "        \"\"\"Creates a list of detection records from the image predictions.\n",
        "        \"\"\"\n",
        "        raw_detections = self._model(x)\n",
        "\n",
        "        def apply_nms(orig_prediction: dict, iou_thresh: float=0.5):\n",
        "            \"\"\"Performs non maximum suppression on the predictions according to their intersection-over-union.\n",
        "            \"\"\"\n",
        "            keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
        "\n",
        "            nms_prediction = orig_prediction\n",
        "            nms_prediction['boxes'] = nms_prediction['boxes'][keep]\n",
        "            nms_prediction['scores'] = nms_prediction['scores'][keep]\n",
        "            nms_prediction['labels'] = nms_prediction['labels'][keep]\n",
        "            return nms_prediction\n",
        "        \n",
        "        def filter_score(orig_prediction: dict, score_thresh: float=0.5):\n",
        "            \"\"\"Filters out model predictions with confidence scores below score_thresh\n",
        "            \"\"\"\n",
        "            keep = orig_prediction['scores'] > score_thresh\n",
        "\n",
        "            filter_prediction = orig_prediction\n",
        "            filter_prediction['boxes'] = filter_prediction['boxes'][keep]\n",
        "            filter_prediction['scores'] = filter_prediction['scores'][keep]\n",
        "            filter_prediction['labels'] = filter_prediction['labels'][keep]\n",
        "            return filter_prediction\n",
        "        \n",
        "        detections = [] \n",
        "        for raw_detection in raw_detections:\n",
        "            raw_detection = apply_nms(raw_detection,0.005)\n",
        "            \n",
        "            # Note that FasterRCNN doesn't return a score for each class, only the predicted class\n",
        "            # DRISE requires a score for each class. We approximate the score for each class\n",
        "            # by dividing the (1.0 - class score) evenly among the other classes.\n",
        "            \n",
        "            raw_detection = filter_score(raw_detection, 0.5)\n",
        "            expanded_class_scores = od_common.expand_class_scores(raw_detection['scores'],\n",
        "                                                                  raw_detection['labels'],\n",
        "                                                                  self._number_of_classes)\n",
        "            detections.append(\n",
        "                od_common.DetectionRecord(\n",
        "                    bounding_boxes=raw_detection['boxes'],\n",
        "                    class_scores=expanded_class_scores,\n",
        "                    objectness_scores=torch.tensor([1.0]*raw_detection['boxes'].shape[0]),\n",
        "                    \n",
        "                )\n",
        "            )\n",
        "        \n",
        "        return detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LcvYe8B6bDK"
      },
      "source": [
        "Now for the fun part - we are ready to run the model to get the wrapped detection outputs and visualize them with saliency maps."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VXx0sJ3TnJP2"
      },
      "source": [
        "# 4. Using the vision-explanation-methods package\n",
        "To generate saliency maps, import DRISE_runner from vision-explanations-method and run get_drise_saliency_map with following parameters.\n",
        "\n",
        "---\n",
        "\n",
        "    imagelocation: str - path of the input image\n",
        "    model: Optional[object] - PyTorch model (default: Faster R-CNN)    \n",
        "    numclasses: int - number of classes model predicts\n",
        "    savename: str - name of output saliency map image\n",
        "    nummasks: int=25 - number of masks to use for saliency\n",
        "    maskres: Tuple[int, int]=(4,4) - resolution of masks before scale-up\n",
        "    maskpadding: Optional[int]=None - how much to pad the mask before cropping\n",
        "    devicechoice: Optional[str]=None - device to use to run D-RISE\n",
        "    wrapperchoice: Optional[object] = PytorchFasterRCNNWrapper - wrapper function for D-RISE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9OKppC6tqDj"
      },
      "outputs": [],
      "source": [
        "from vision_explanation_methods import DRISE_runner as dr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "TfxsvfYytxpA",
        "outputId": "0e55e71c-5dd2-4c8f-8a90-6ebbcc35163b"
      },
      "outputs": [],
      "source": [
        "res = dr.get_drise_saliency_map(imagelocation='odFridgeObjects/images/17.jpg',\n",
        "                                    model=PytorchFasterRCNNWrapper(\n",
        "                                          model=model,\n",
        "                                          number_of_classes=5),\n",
        "                                    numclasses=5,\n",
        "                                    savename='outputmap.jpg',\n",
        "                                    max_figures=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "O00011F2HhR2"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "a458a1cdfa0a46c263c0e8806e23702c968c1bf3ff7084a5ef5fe051d9653aa4"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a48079bd32c4b68af373753a6e89524": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0d9c2dda8224d08a00347ad85761ded",
              "IPY_MODEL_75802c4660524073a07a6f5115e2a375",
              "IPY_MODEL_d13c6cf681e848558e5b2e0f542c5f60"
            ],
            "layout": "IPY_MODEL_303743a5454f44f49ab3d4701c04beb5"
          }
        },
        "1966e4a247fb47e19a31901696a62dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "303743a5454f44f49ab3d4701c04beb5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52f85758e0064f46a414a51506e9040b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54ab686aacb244af8e26d86a628815e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75802c4660524073a07a6f5115e2a375": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52f85758e0064f46a414a51506e9040b",
            "max": 167502836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8986f48e2e1f47988cd55dfa68fef651",
            "value": 167502836
          }
        },
        "8986f48e2e1f47988cd55dfa68fef651": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a36adc2dccd24407a0dccd854905f1c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7060c74f47e4c1a8d3acb7f4159ee93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d13c6cf681e848558e5b2e0f542c5f60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7060c74f47e4c1a8d3acb7f4159ee93",
            "placeholder": "​",
            "style": "IPY_MODEL_a36adc2dccd24407a0dccd854905f1c3",
            "value": " 160M/160M [00:04&lt;00:00, 33.9MB/s]"
          }
        },
        "f0d9c2dda8224d08a00347ad85761ded": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54ab686aacb244af8e26d86a628815e8",
            "placeholder": "​",
            "style": "IPY_MODEL_1966e4a247fb47e19a31901696a62dbe",
            "value": "100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
